# -*- coding: utf-8 -*-
"""
ç•ªå‰§æ¨èè„šæœ¬
æ¯6å°æ—¶è‡ªåŠ¨æ‰§è¡Œï¼ˆ8:00, 14:00, 20:00, 2:00ï¼‰ï¼Œä»RSSæºè·å–æœ€æ–°ç•ªå‰§ä¿¡æ¯ï¼Œä½¿ç”¨å¤§æ¨¡å‹æ•´ç†åä¿å­˜
"""
import json
import os
import requests
import feedparser
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
import schedule
import time
import logging
import sys

# å¤„ç†å¯¼å…¥é—®é¢˜ï¼šæ”¯æŒç›´æ¥è¿è¡Œå’Œä½œä¸ºæ¨¡å—å¯¼å…¥
try:
    from components.rss.comic_json import txt_to_json
except ImportError:
    # å¦‚æœä½œä¸ºæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç›¸å¯¹å¯¼å…¥æˆ–æ·»åŠ è·¯å¾„
    try:
        from comic_json import txt_to_json
    except ImportError:
        # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
        project_root = Path(__file__).parent.parent.parent
        if str(project_root) not in sys.path:
            sys.path.insert(0, str(project_root))
        from components.rss.comic_json import txt_to_json

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('comic_recommend.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
env_path = Path(__file__).parent.parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

# DEEPSEEK API é…ç½®
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY', '')
DEEPSEEK_BASE_URL = os.getenv('DEEPSEEK_BASE_URL', 'https://api.deepseek.com')
DEEPSEEK_MODEL = os.getenv('DEEPSEEK_MODEL', 'deepseek-chat')


def load_rss_sources():
    """ä» src.json åŠ è½½ RSS æº"""
    src_file = Path(__file__).parent / 'src.json'
    try:
        with open(src_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        rss_urls = []
        for item in data:
            if 'ç•ªå‰§' in item:
                rss_urls.append(item['ç•ªå‰§'])
        logger.info(f"æˆåŠŸåŠ è½½ {len(rss_urls)} ä¸ª RSS æº")
        return rss_urls
    except Exception as e:
        logger.error(f"åŠ è½½ RSS æºå¤±è´¥: {e}")
        return []


def fetch_rss_items(rss_url, max_items=20):
    """ä» RSS æºè·å–å‰ N æ¡ä¿¡æ¯"""
    try:
        logger.info(f"æ­£åœ¨è·å– RSS æº: {rss_url}")
        response = requests.get(rss_url, timeout=30)
        response.raise_for_status()
        
        # è§£æ RSS
        feed = feedparser.parse(response.content)
        
        items = []
        for entry in feed.entries[:max_items]:
            item = {
                'title': entry.get('title', ''),
                'link': entry.get('link', ''),
                'published': entry.get('published', ''),
                'summary': entry.get('summary', '')[:200] if entry.get('summary') else ''  # é™åˆ¶é•¿åº¦
            }
            items.append(item)
        
        logger.info(f"æˆåŠŸè·å– {len(items)} æ¡ RSS æ¡ç›®")
        return items
    except Exception as e:
        logger.error(f"è·å– RSS æºå¤±è´¥ {rss_url}: {e}")
        return []


def call_deepseek_api(rss_items):
    """è°ƒç”¨ DEEPSEEK API æ•´ç†ç•ªå‰§ä¿¡æ¯"""
    if not DEEPSEEK_API_KEY:
        raise ValueError("DEEPSEEK_API_KEY æœªé…ç½®ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
    
    # æ„å»ºæç¤ºè¯
    items_text = ""
    for i, item in enumerate(rss_items, 1):
        items_text += f"{i}. æ ‡é¢˜: {item['title']}\n"
        items_text += f"   æ›´æ–°æ—¶é—´: {item['published']}\n"
        items_text += f"   URL: {item['link']}\n"
        if item['summary']:
            items_text += f"   ç®€ä»‹: {item['summary']}\n"
        items_text += "\n"
    
    prompt = f"""è¯·å¸®æˆ‘æ•´ç†ä»¥ä¸‹ç•ªå‰§ä¿¡æ¯ï¼Œæå–ä»Šæ—¥æ¨èçš„ç•ªå‰§ã€‚

ã€é‡è¦æ ¼å¼è¦æ±‚ã€‘
è¾“å‡ºæ ¼å¼å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªç•ªå‰§ï¼Œæ ¼å¼ä¸ºï¼š
ç•ªå‰§åç§° - æ›´æ–°æ—¶é—´ - URLåœ°å€

ç¤ºä¾‹ï¼š
è¿›å‡»çš„å·¨äºº æœ€ç»ˆå­£ - 2024-01-15 12:00:00 - https://example.com/episode1
é¬¼ç­ä¹‹åˆƒ æ— é™åˆ—è½¦ç¯‡ - 2024-01-14 18:30:00 - https://example.com/episode2

ã€ç­›é€‰è¦æ±‚ã€‘
1. åªé€‰æ‹©æœ€å€¼å¾—æ¨èçš„ç•ªå‰§ï¼Œæ•°é‡æ§åˆ¶åœ¨5-10ä¸ªä¹‹é—´
2. ç»å¯¹ä¸è¦é‡å¤ï¼Œå¦‚æœæœ‰ç›¸åŒçš„ç•ªå‰§ï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€æ¡
3. ä¼˜å…ˆæ¨èæœ€æ–°æ›´æ–°çš„ç•ªå‰§
4. åªè¾“å‡ºæ¨èç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—
5. ä¸¥æ ¼æŒ‰ç…§"ç•ªå‰§åç§° - æ›´æ–°æ—¶é—´ - URLåœ°å€"æ ¼å¼è¾“å‡ºï¼Œä½¿ç”¨ä¸­æ–‡çŸ­æ¨ªçº¿" - "åˆ†éš”
6. ç•ªå‰§åç§°åˆ«å¼„é”™

ä»¥ä¸‹æ˜¯ RSS æºè·å–çš„ç•ªå‰§ä¿¡æ¯ï¼š

{items_text}

è¯·æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¦æ±‚è¾“å‡ºæ¨èç»“æœï¼ˆåªè¾“å‡ºæ¨èåˆ—è¡¨ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š"""

    try:
        client = OpenAI(
            base_url=DEEPSEEK_BASE_URL,
            api_key=DEEPSEEK_API_KEY,
        )
        
        logger.info("æ­£åœ¨è°ƒç”¨ DEEPSEEK API æ•´ç†ç•ªå‰§ä¿¡æ¯...")
        response = client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç•ªå‰§æ¨èåŠ©æ‰‹ï¼Œæ“…é•¿æ•´ç†å’Œç­›é€‰ä¼˜è´¨çš„ç•ªå‰§å†…å®¹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3  # ä½¿ç”¨è¾ƒä½æ¸©åº¦ä¿è¯è¾“å‡ºæ ¼å¼ä¸€è‡´æ€§
        )
        
        result = response.choices[0].message.content
        logger.info("æˆåŠŸè·å–å¤§æ¨¡å‹æ•´ç†ç»“æœ")
        return result.strip()
    except Exception as e:
        logger.error(f"è°ƒç”¨ DEEPSEEK API å¤±è´¥: {e}")
        raise


def save_recommendation(content):
    """ä¿å­˜æ¨èå†…å®¹åˆ°æ–‡ä»¶"""
    # è·å–å½“å‰æ—¥æœŸ
    today = datetime.now()
    date_str = today.strftime("%Y-%m-%d")
    
    # åˆ›å»º data ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    data_dir = Path(__file__).parent / 'data'
    data_dir.mkdir(exist_ok=True)
    
    # æ–‡ä»¶è·¯å¾„ï¼šå¹´ä»½-æœˆä»½-æ—¥æœŸ.txt
    file_path = data_dir / f"{date_str}.txt"
    
    # æ„å»ºæ–‡ä»¶å†…å®¹ï¼šå¼€å¤´æ˜¯æ›´æ–°æ—¶é—´ï¼Œç„¶åæ˜¯ç•ªå‰§ä¿¡æ¯
    current_time = today.strftime("%Y-%m-%d %H:%M:%S")
    file_content = f"æ›´æ–°æ—¶é—´: {current_time}\n\n{content}\n"
    
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(file_content)
        logger.info(f"æ¨èå†…å®¹å·²ä¿å­˜åˆ°: {file_path}")
        return file_path
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        raise


def run_recommendation():
    """æ‰§è¡Œå®Œæ•´çš„æ¨èæµç¨‹"""
    from datetime import datetime
    
    try:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        logger.info("=" * 70)
        logger.info(f"ğŸ“º [{timestamp}] å¼€å§‹æ‰§è¡Œç•ªå‰§æ¨èä»»åŠ¡...")
        logger.info("=" * 70)
        
        # 1. åŠ è½½ RSS æº
        rss_urls = load_rss_sources()
        if not rss_urls:
            logger.warning("   âš ï¸  æ²¡æœ‰æ‰¾åˆ° RSS æºï¼Œä»»åŠ¡ç»“æŸ")
            logger.info("=" * 70 + "\n")
            return
        
        logger.info(f"   ğŸ“¡ å·²åŠ è½½ {len(rss_urls)} ä¸ª RSS æº")
        
        # 2. è·å–æ‰€æœ‰ RSS æºçš„ä¿¡æ¯
        all_items = []
        for rss_url in rss_urls:
            items = fetch_rss_items(rss_url, max_items=20)
            all_items.extend(items)
            logger.info(f"   ğŸ“¥ ä» {rss_url} è·å–äº† {len(items)} æ¡ä¿¡æ¯")
        
        if not all_items:
            logger.warning("   âš ï¸  æœªèƒ½è·å–åˆ°ä»»ä½• RSS æ¡ç›®ï¼Œä»»åŠ¡ç»“æŸ")
            logger.info("=" * 70 + "\n")
            return
        
        logger.info(f"   ğŸ“Š æ€»å…±è·å– {len(all_items)} æ¡ RSS æ¡ç›®")
        
        # 3. è°ƒç”¨å¤§æ¨¡å‹æ•´ç†
        logger.info("   ğŸ¤– æ­£åœ¨è°ƒç”¨å¤§æ¨¡å‹æ•´ç†æ¨èå†…å®¹...")
        recommendation_content = call_deepseek_api(all_items)
        
        # 4. ä¿å­˜åˆ°æ—¥æœŸæ–‡ä»¶
        logger.info("   ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨èå†…å®¹...")
        save_recommendation(recommendation_content)
        
        # 5. å°†txtæ–‡ä»¶è½¬æ¢ä¸ºjsonæ ¼å¼
        try:
            data_dir = Path(__file__).parent / 'data'
            json_data = txt_to_json(data_dir=data_dir, auto_save=True)
            logger.info(f"   âœ… JSONæ–‡ä»¶å·²æˆåŠŸç”Ÿæˆ: {json_data.get('date', 'unknown')}.json")
        except Exception as e:
            logger.error(f"   âŒ è½¬æ¢JSONæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        
        logger.info(f"\nâœ… [{timestamp}] ç•ªå‰§æ¨èä»»åŠ¡æ‰§è¡Œå®Œæˆ")
        logger.info("=" * 70 + "\n")
        
    except Exception as e:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        logger.error("=" * 70)
        logger.error(f"âŒ [{timestamp}] æ‰§è¡Œæ¨èä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        logger.error("=" * 70 + "\n", exc_info=True)


def schedule_job():
    """è°ƒåº¦ä»»åŠ¡ï¼šæ¯6å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼ˆ8:00, 14:00, 20:00, 2:00ï¼‰"""
    # è®¾ç½®å››ä¸ªæ—¶é—´ç‚¹æ‰§è¡Œ
    schedule.every().day.at("08:00").do(run_recommendation)
    schedule.every().day.at("14:00").do(run_recommendation)
    schedule.every().day.at("20:00").do(run_recommendation)
    schedule.every().day.at("02:00").do(run_recommendation)
    logger.info("å·²è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼šæ¯6å°æ—¶æ‰§è¡Œç•ªå‰§æ¨èï¼ˆ8:00, 14:00, 20:00, 2:00ï¼‰")
    
    # ä¿æŒç¨‹åºè¿è¡Œ
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡


def start_schedule_in_thread():
    """åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
    import threading
    from datetime import datetime
    
    def run_schedule():
        # è®¾ç½®å››ä¸ªæ—¶é—´ç‚¹æ‰§è¡Œ
        schedule.every().day.at("08:00").do(run_recommendation)
        schedule.every().day.at("14:00").do(run_recommendation)
        schedule.every().day.at("20:00").do(run_recommendation)
        schedule.every().day.at("02:00").do(run_recommendation)
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        logger.info(f"   â° [{timestamp}] å®šæ—¶ä»»åŠ¡å·²æ³¨å†Œ: æ¯å¤© 8:00ã€14:00ã€20:00ã€2:00 æ‰§è¡Œ")
        
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    thread = threading.Thread(target=run_schedule, daemon=True)
    thread.start()
    return thread


if __name__ == "__main__":
    import sys
    
    # å¦‚æœä¼ å…¥å‚æ•° --nowï¼Œç«‹å³æ‰§è¡Œä¸€æ¬¡
    if len(sys.argv) > 1 and sys.argv[1] == '--now':
        run_recommendation()
    else:
        # å¦åˆ™å¯åŠ¨å®šæ—¶ä»»åŠ¡
        logger.info("å¯åŠ¨å®šæ—¶ä»»åŠ¡æ¨¡å¼")
        logger.info("å¦‚éœ€ç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼Œè¯·ä½¿ç”¨: python comic_recommend.py --now")
        schedule_job()

