# -*- coding: utf-8 -*-
"""
Lumina æ™ºèƒ½ä½“å®ç°
åŸºäº Google Gemini Flash 3 çš„ Agent
"""
import json
import time
from typing import List, Dict, Any, Optional, Generator
import google.genai as genai
from config.llm.base.agent import BaseAgent
from config.llm.base.settings import GEMINI_API_KEY, GEMINI_MODEL, TEMPERATURE
from config.llm.base.prompts.utils import get_system_prompt_with_time
from config.llm.lumina.prompt import SYSTEM_PROMPT_BASE
from config.llm.base.history import save_message
from tools import TOOLS, execute_tool


class LuminaAgent(BaseAgent):
    """
    Lumina æ™ºèƒ½ä½“
    åŸºäº Google Gemini Flash 3 çš„ Agent å®ç°ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨
    """
    
    def __init__(self):
        super().__init__(
            name="Lumina",
            description="åŸºäº Google Gemini Flash 3 çš„æ™ºèƒ½ä½“ï¼Œè¿›è¡Œæ¯æ—¥ä¸€æ–‡çš„åˆ›ä½œ"
        )
        self._client = None
        self._model = GEMINI_MODEL
        self._max_tokens = 8192
    
    def _create_client(self):
        """åˆ›å»º Gemini API å®¢æˆ·ç«¯"""
        if self._client is None:
            if not GEMINI_API_KEY:
                raise ValueError("GEMINI_API_KEY æœªé…ç½®ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®")
            
            # åˆ›å»º Gemini API å®¢æˆ·ç«¯ï¼ˆæ–°åŒ…ä½¿ç”¨å®¢æˆ·ç«¯æ¨¡å¼ï¼‰
            # æ–°åŒ…ä»ç„¶æ”¯æŒ GenerativeModelï¼Œä½†éœ€è¦é€šè¿‡å®¢æˆ·ç«¯æ¥é…ç½®
            self._client = genai.Client(api_key=GEMINI_API_KEY)
            # ä¸ºäº†å…¼å®¹ï¼ŒåŒæ—¶è®¾ç½®å…¨å±€é…ç½®ï¼ˆå¦‚æœæ–°åŒ…æ”¯æŒï¼‰
            try:
                genai.configure(api_key=GEMINI_API_KEY)
            except AttributeError:
                # æ–°åŒ…å¯èƒ½ä¸æ”¯æŒ configureï¼Œä½¿ç”¨å®¢æˆ·ç«¯å³å¯
                pass
        return self._client
    
    def get_system_prompt(self, location: Optional[Dict[str, float]] = None) -> str:
        """
        è·å–ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«æ—¶é—´ä¿¡æ¯ï¼‰
        
        Args:
            location: ç”¨æˆ·ä½ç½®ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            ç³»ç»Ÿæç¤ºè¯å­—ç¬¦ä¸²
        """
        return get_system_prompt_with_time(SYSTEM_PROMPT_BASE.strip(), location)
    
    def _convert_tools_to_gemini_format(self) -> List[Dict[str, Any]]:
        """
        å°†å·¥å…·å®šä¹‰è½¬æ¢ä¸º Gemini æ ¼å¼
        
        Returns:
            Gemini æ ¼å¼çš„å·¥å…·å®šä¹‰åˆ—è¡¨
        """
        function_declarations = []
        for tool_name, tool_info in TOOLS.items():
            # è¿‡æ»¤æ‰ send_emoji å·¥å…·ï¼ˆLumina ä¸æ”¯æŒè¡¨æƒ…åŒ…åŠŸèƒ½ï¼‰
            if tool_name == "send_emoji":
                continue
            
            # è½¬æ¢å‚æ•°å®šä¹‰
            properties = {}
            required = []
            
            params = tool_info.get("parameters", {}).get("properties", {})
            for param_name, param_info in params.items():
                properties[param_name] = {
                    "type": param_info.get("type", "string"),
                    "description": param_info.get("description", "")
                }
                if param_name in tool_info.get("parameters", {}).get("required", []):
                    required.append(param_name)
            
            function_declarations.append({
                "name": tool_name,
                "description": tool_info["description"],
                "parameters": {
                    "type": "object",
                    "properties": properties,
                    "required": required
                }
            })
        
        # Gemini SDK æœŸæœ›çš„æ ¼å¼æ˜¯ä¸€ä¸ªåŒ…å« function_declarations çš„å­—å…¸
        return [{"function_declarations": function_declarations}] if function_declarations else []
    
    def get_tools(self) -> List[Dict[str, Any]]:
        """
        è·å–å·¥å…·åˆ—è¡¨ï¼ˆOpenAI æ ¼å¼ï¼Œç”¨äºå…¼å®¹åŸºç±»æ¥å£ï¼‰
        
        Returns:
            å·¥å…·å®šä¹‰åˆ—è¡¨
        """
        tools = []
        for tool_name, tool_info in TOOLS.items():
            # è¿‡æ»¤æ‰ send_emoji å·¥å…·ï¼ˆLumina ä¸æ”¯æŒè¡¨æƒ…åŒ…åŠŸèƒ½ï¼‰
            if tool_name == "send_emoji":
                continue
            
            tools.append({
                "type": "function",
                "function": {
                    "name": tool_name,
                    "description": tool_info["description"],
                    "parameters": tool_info.get("parameters", {})
                }
            })
        return tools
    
    def _convert_messages_to_gemini_format(self, messages: List[Dict[str, Any]], system_prompt: str) -> List[Dict[str, Any]]:
        """
        å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸º Gemini æ ¼å¼
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼ˆOpenAI æ ¼å¼ï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯
        
        Returns:
            Gemini æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        """
        gemini_messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ä½œä¸ºç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        if system_prompt:
            gemini_messages.append({
                "role": "user",
                "parts": [{"text": system_prompt}]
            })
            gemini_messages.append({
                "role": "model",
                "parts": [{"text": "å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚"}]
            })
        
        # è½¬æ¢æ¶ˆæ¯
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            # è·³è¿‡ system è§’è‰²ï¼ˆå·²åœ¨å‰é¢å¤„ç†ï¼‰
            if role == "system":
                continue
            
            # å¤„ç† tool è§’è‰²ï¼ˆå·¥å…·æ‰§è¡Œç»“æœï¼‰
            if role == "tool":
                # Gemini ä½¿ç”¨ function_response æ ¼å¼
                tool_name = msg.get("name", "")
                tool_result = msg.get("content", "")
                tool_call_id = msg.get("tool_call_id", "")
                
                gemini_messages.append({
                    "role": "user",
                    "parts": [{
                        "function_response": {
                            "name": tool_name,
                            "response": tool_result
                        }
                    }]
                })
                continue
            
            # å¤„ç† assistant è§’è‰²ï¼ˆå¯èƒ½åŒ…å«å·¥å…·è°ƒç”¨ï¼‰
            if role == "assistant":
                parts = []
                
                # æ·»åŠ æ–‡æœ¬å†…å®¹
                if content:
                    parts.append({"text": content})
                
                # æ·»åŠ å·¥å…·è°ƒç”¨
                tool_calls = msg.get("tool_calls", [])
                for tool_call in tool_calls:
                    function_name = tool_call.get("function", {}).get("name", "")
                    function_args = tool_call.get("function", {}).get("arguments", "{}")
                    
                    try:
                        # è§£æå‚æ•°
                        if isinstance(function_args, str):
                            args_dict = json.loads(function_args)
                        else:
                            args_dict = function_args
                    except (json.JSONDecodeError, TypeError):
                        args_dict = {}
                    
                    parts.append({
                        "function_call": {
                            "name": function_name,
                            "args": args_dict
                        }
                    })
                
                if parts:
                    gemini_messages.append({
                        "role": "model",
                        "parts": parts
                    })
                continue
            
            # å¤„ç† user è§’è‰²
            if role == "user":
                gemini_messages.append({
                    "role": "user",
                    "parts": [{"text": content}]
                })
        
        return gemini_messages
    
    def _handle_final_response(self, final_response: str, email: Optional[str], session_id: str, pending_favorite_image: Optional[Dict[str, Any]]):
        """
        å¤„ç†æœ€ç»ˆå“åº”ï¼ˆå‘é€å®Œæˆæ ‡è®°ã€æ”¶è—å›¾ç‰‡ï¼‰
        
        Args:
            final_response: æœ€ç»ˆå“åº”å†…å®¹
            email: ç”¨æˆ·é‚®ç®±
            session_id: ä¼šè¯ID
            pending_favorite_image: å¾…å‘é€çš„æ”¶è—å›¾ç‰‡
        
        Yields:
            str: SSEæ ¼å¼çš„å“åº”æ•°æ®
        """
        if final_response:
            save_message(email, "assistant", final_response, session_id)
        
        # å‘é€å®Œæˆæ ‡è®°
        yield f"data: {json.dumps({'content': '', 'done': True}, ensure_ascii=False)}\n\n"
        
        # å‘é€æ”¶è—å›¾ç‰‡
        if pending_favorite_image:
            print(f"â³ [åç«¯] æµå¼è¾“å‡ºå®Œæˆï¼Œç­‰å¾…1ç§’åå‘é€æ”¶è—å›¾ç‰‡...")
            time.sleep(1)
            print(f"ğŸ“¤ [åç«¯] å‡†å¤‡å‘é€æ”¶è—å›¾ç‰‡äº‹ä»¶åˆ°å‰ç«¯")
            print(f"ğŸ“¤ [åç«¯] æ”¶è—å›¾ç‰‡äº‹ä»¶æ•°æ®: {json.dumps(pending_favorite_image, ensure_ascii=False)}")
            yield f"data: {json.dumps(pending_favorite_image, ensure_ascii=False)}\n\n"
    
    def stream_response(
        self,
        messages: List[Dict[str, Any]],
        session_id: str,
        location: Optional[Dict[str, float]] = None,
        email: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        æµå¼ç”Ÿæˆå“åº”ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            session_id: ä¼šè¯ID
            location: ç”¨æˆ·ä½ç½®ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            email: ç”¨æˆ·é‚®ç®±ï¼ˆç”¨äºå†å²è®°å½•å­˜å‚¨ï¼‰
        
        Yields:
            str: SSEæ ¼å¼çš„æµå¼å“åº”æ•°æ®
        """
        client = self._create_client()
        user_location = location
        
        # æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°ï¼Œé¿å…æ— é™å¾ªç¯
        max_tool_calls = 5
        tool_call_count = 0
        pending_favorite_image = None
        accumulated_content = ""
        
        # åˆå§‹åŒ–æ¶ˆæ¯åˆ—è¡¨
        full_messages = list(messages)
        
        while tool_call_count < max_tool_calls:
            # æ¯æ¬¡å¾ªç¯éƒ½é‡æ–°ç”Ÿæˆç³»ç»Ÿæç¤ºè¯ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„æ—¶é—´ä¿¡æ¯
            system_prompt = self.get_system_prompt(location)
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            gemini_messages = self._convert_messages_to_gemini_format(full_messages, system_prompt)
            
            # è·å–å·¥å…·å®šä¹‰ï¼ˆGemini æ ¼å¼ï¼‰
            gemini_tools = self._convert_tools_to_gemini_format()
            
            try:
                # è·å–å·¥å…·å®šä¹‰ï¼ˆGemini æ ¼å¼ï¼‰
                gemini_tools = self._convert_tools_to_gemini_format() if TOOLS else None
                
                # æ–°ç‰ˆæœ¬çš„ google-genai ä½¿ç”¨ Client å’Œ models.generate_content()
                # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
                last_user_message = gemini_messages[-1] if gemini_messages else None
                if not last_user_message or last_user_message.get("role") != "user":
                    break
                
                # å‡†å¤‡ç”Ÿæˆå†…å®¹å‚æ•°
                generate_config = {
                    "temperature": TEMPERATURE,
                    "max_output_tokens": self._max_tokens,
                }
                
                # å¦‚æœæœ‰å·¥å…·ï¼Œæ·»åŠ åˆ°é…ç½®ä¸­
                if gemini_tools and len(gemini_tools) > 0 and len(gemini_tools[0].get("function_declarations", [])) > 0:
                    generate_config["tools"] = gemini_tools
                
                # ä½¿ç”¨ Client ç”Ÿæˆå†…å®¹ï¼ˆæµå¼ï¼‰
                # æ–°ç‰ˆæœ¬ API ä½¿ç”¨ generate_content_stream
                response = client.models.generate_content_stream(
                    model=self._model,
                    contents=gemini_messages,
                    config=generate_config
                )
                
                full_response = ""
                tool_calls = []
                content_before_tool_call = ""
                has_tool_call = False
                
                # å¤„ç†æµå¼å“åº”
                for chunk in response:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æœ¬å†…å®¹
                    if hasattr(chunk, 'text') and chunk.text:
                        chunk_text = chunk.text
                        full_response += chunk_text
                        if not has_tool_call:
                            content_before_tool_call += chunk_text
                            yield f"data: {json.dumps({'content': chunk_text, 'done': False}, ensure_ascii=False)}\n\n"
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å‡½æ•°è°ƒç”¨ï¼ˆGemini SDK æ ¼å¼ï¼‰
                    if hasattr(chunk, 'candidates') and chunk.candidates:
                        for candidate in chunk.candidates:
                            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                                for part in candidate.content.parts:
                                    if hasattr(part, 'function_call') and part.function_call:
                                        has_tool_call = True
                                        func_call = part.function_call
                                        if func_call and hasattr(func_call, 'name') and func_call.name:
                                            tool_calls.append({
                                                "id": func_call.name + "_" + str(tool_call_count),
                                                "name": func_call.name,
                                                "args": dict(func_call.args) if hasattr(func_call, 'args') and hasattr(func_call.args, '__iter__') and not isinstance(func_call.args, str) else (func_call.args if hasattr(func_call, 'args') else {})
                                            })
                
                # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå·¥å…·
                if tool_calls:
                    tool_call_count += 1
                    accumulated_content += content_before_tool_call
                    
                    # æ„å»ºå·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆOpenAI æ ¼å¼ï¼Œç”¨äºå†å²è®°å½•ï¼‰
                    tool_calls_info = []
                    tool_results_parts = []
                    
                    for tool_call in tool_calls:
                        tool_name = tool_call["name"]
                        tool_args = tool_call.get("args", {})
                        
                        # å¦‚æœæ˜¯get_weatherå·¥å…·ä¸”æ²¡æœ‰æä¾›ä½ç½®å‚æ•°ï¼Œä½¿ç”¨ç”¨æˆ·ä½ç½®
                        if tool_name == "get_weather" and user_location:
                            if not tool_args.get("location") and not (tool_args.get("latitude") and tool_args.get("longitude")):
                                tool_args["latitude"] = user_location.get("latitude")
                                tool_args["longitude"] = user_location.get("longitude")
                        
                        # æ‰§è¡Œå·¥å…·
                        tool_result = self.execute_tool(tool_name, tool_args)
                        
                        # ç‰¹æ®Šå¤„ç† send_favorite_image å·¥å…·
                        if tool_name == "send_favorite_image" and isinstance(tool_result, dict) and tool_result.get("sent"):
                            print(f"ğŸ“¤ [åç«¯] æ£€æµ‹åˆ°æ”¶è—å›¾ç‰‡å·¥å…·è°ƒç”¨ï¼Œå°†å»¶è¿Ÿåˆ°æµå¼è¾“å‡ºå®Œæˆåå‘é€")
                            pending_favorite_image = {
                                "type": "favorite_image",
                                "image_filename": tool_result.get("image_filename"),
                                "image_url": tool_result.get("image_url"),
                                "description": tool_result.get("description")
                            }
                        
                        # ä¿å­˜å·¥å…·æ‰§è¡Œç»“æœåˆ°å†å²è®°å½•
                        if email:
                            save_message(
                                email,
                                "tool",
                                json.dumps(tool_result, ensure_ascii=False),
                                session_id,
                                tool_call_id=tool_call["id"],
                                tool_name=tool_name
                            )
                        
                        # æ„å»ºå·¥å…·ç»“æœï¼ˆGemini æ ¼å¼ï¼‰
                        tool_results_parts.append({
                            "function_response": {
                                "name": tool_name,
                                "response": json.dumps(tool_result, ensure_ascii=False)
                            }
                        })
                        
                        # æ„å»ºå·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆOpenAI æ ¼å¼ï¼‰
                        tool_calls_info.append({
                            "id": tool_call["id"],
                            "type": "function",
                            "function": {
                                "name": tool_name,
                                "arguments": json.dumps(tool_args, ensure_ascii=False)
                            }
                        })
                    
                    # å°†å·¥å…·è°ƒç”¨æ·»åŠ åˆ°æ¶ˆæ¯å†å²ï¼ˆOpenAI æ ¼å¼ï¼‰
                    full_messages.append({
                        "role": "assistant",
                        "content": full_response if full_response else None,
                        "tool_calls": tool_calls_info
                    })
                    
                    # ä¿å­˜å·¥å…·è°ƒç”¨ä¿¡æ¯åˆ°å†å²è®°å½•
                    if email:
                        save_message(
                            email,
                            "assistant",
                            full_response if full_response else "",
                            session_id,
                            tool_calls=tool_calls_info
                        )
                    
                    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²ï¼ˆOpenAI æ ¼å¼ï¼‰
                    for tool_call_info in tool_calls_info:
                        tool_name = tool_call_info["function"]["name"]
                        tool_result_str = None
                        for part in tool_results_parts:
                            if part["function_response"]["name"] == tool_name:
                                tool_result_str = part["function_response"]["response"]
                                break
                        
                        full_messages.append({
                            "role": "tool",
                            "name": tool_name,
                            "content": tool_result_str or "",
                            "tool_call_id": tool_call_info["id"]
                        })
                    
                    # å°†å·¥å…·ç»“æœæ·»åŠ åˆ° Gemini æ¶ˆæ¯å†å²
                    gemini_messages.append({
                        "role": "user",
                        "parts": tool_results_parts
                    })
                    
                    accumulated_content = ""
                    continue
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ­£å¸¸è¿”å›å“åº”
                    final_response = accumulated_content + full_response
                    yield from self._handle_final_response(final_response, email, session_id, pending_favorite_image)
                    accumulated_content = ""
                    break
                    
            except Exception as e:
                print(f"âŒ [åç«¯] Gemini API è°ƒç”¨å¤±è´¥: {e}")
                error_msg = f"æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼š{str(e)}"
                yield f"data: {json.dumps({'content': error_msg, 'done': False}, ensure_ascii=False)}\n\n"
                yield from self._handle_final_response(error_msg, email, session_id, pending_favorite_image)
                return
        
        # å¦‚æœè¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œè¿”å›æœ€ç»ˆå“åº”
        if tool_call_count >= max_tool_calls:
            final_response = accumulated_content + full_response if 'full_response' in locals() else accumulated_content
            yield from self._handle_final_response(final_response, email, session_id, pending_favorite_image)
    
    def execute_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        """
        æ‰§è¡Œå·¥å…·è°ƒç”¨
        
        Args:
            tool_name: å·¥å…·åç§°
            arguments: å·¥å…·å‚æ•°
        
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        return execute_tool(tool_name, arguments)

